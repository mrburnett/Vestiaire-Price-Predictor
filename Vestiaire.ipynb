{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjeiuW9j-uXM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# PROJECT GOALS:\n",
        "''' Create a neural network that runs a regression analysis on prices of different clothings items from different brands.\n",
        "    The network will, given an item and the brand, recommend a price based on other user's listings.\n",
        "    Add-ons:\n",
        "    - weigh 'sold' items, 'reserved' items, and 'unsold' items differently'''\n",
        "\n",
        "# load in data\n",
        "vestiaire_df = pd.read_csv('/content/drive/MyDrive/Vestiaire Model/vestiaire.csv', low_memory=False)\n",
        "\n",
        "# clean and tokenize data\n",
        "target_df = vestiaire_df[['product_type', 'product_description', 'product_material', 'product_gender_target', 'brand_name', 'price_usd', 'brand_id', 'product_condition']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Q31l6m-eyBpw"
      },
      "outputs": [],
      "source": [
        "clothing_dict = {\n",
        "    'shirt': ['shirt', 'tshirt', 't-shirt', 'tank top', 'top', 'tee', 'blouse', 'polo', 'vest', 'tunic',  'twin-set', 'camisole'],\n",
        "    'sweater': ['sweater', 'jumper', 'cardigan', 'knitwear', 'pull'],\n",
        "    'skirt': ['skirt', 'pareo'],\n",
        "    'shorts': ['shorts', 'short', 'bermuda'],\n",
        "    'pants': ['pants', 'jeans', 'jean', 'trousers', 'slacks', 'leggings', 'harem'],\n",
        "    'jacket': ['jacket', 'coat', 'blazer', 'windbreaker', 'parka', 'puffer', 'cape', 'poncho', 'caban', 'trench'],\n",
        "    'dress': ['dress', 'gown', 'sundress'],\n",
        "    'jumpsuit': ['jumpsuit', 'overall'],\n",
        "    'undergarment': ['undergarment', 'bra', 'lingerie', 'slip', 'bustier', 'corset', 'tight', 'string'],\n",
        "    'swimwear': ['swimwear', 'swimsuit'],\n",
        "    'suit': ['suit'],\n",
        "    'shoes': ['sneakers', 'boots', 'heels', 'heel', 'sandals', 'sandal', 'trainers', 'flats', 'flat', 'mules', 'espadrilles', 'flip flops', 'lace ups', 'escarpins a paillettes', ' leather'],\n",
        "    'accessories': ['accessories', 'accessory', 'jewelry', 'jewellery', 'hat', 'gloves', 'watch', 'belt', 'hankerchief', 'cuff links', 'tie', 'sunglasses', 'cufflinks', 'scarf', 'pocket square', 'wallet', 'ring', 'purse', 'clutch', 'neckerchief', 'beanie', 'stole', 'iphone case', 'ipad', 'glasses', 'handkerchief', 'mittens', 'cheche', 'cap', 'beret', 'choker', 'panama', 'diary', 'lifestyle', 'page/nom_singulier/31']\n",
        "}\n",
        "\n",
        "def map_clothing(typ, dic):\n",
        "  for key, vals in dic.items():\n",
        "    if any(val in typ for val in vals):\n",
        "      return key\n",
        "  return typ\n",
        "\n",
        "def standardize_data(df):\n",
        "  # work on copy of the original dataframe\n",
        "  df = df.copy()\n",
        "\n",
        "  # make every str column lowercase\n",
        "  for col in df.columns:\n",
        "      if df[col].dtype == 'object':\n",
        "          df.loc[:, col] = df[col].str.lower()\n",
        "\n",
        "  # apply clothing dictionary\n",
        "  df['clothing_cat'] = df['product_type'].apply(map_clothing, args=(clothing_dict,))\n",
        "\n",
        "  # fix any nan in gender/material category\n",
        "  df['product_gender_target'].fillna('Unisex', inplace=True) # from data there arent many/any nan, safe to possibly mislabel some as unisex instead of trying to find gender from description\n",
        "  df['product_material'].fillna('not specified', inplace=True)\n",
        "\n",
        "  # drop any rows with no price\n",
        "  df.dropna(subset=['price_usd'], inplace=True)\n",
        "\n",
        "  # encode categorical data\n",
        "  cat_encoder = LabelEncoder()\n",
        "  cond_encoder = LabelEncoder()\n",
        "  gen_encoder = LabelEncoder()\n",
        "  brand_encoder = LabelEncoder()\n",
        "  material_encoder = LabelEncoder()\n",
        "\n",
        "  # create encoders for each variable - separate step so i can use encoders later\n",
        "  cat_encoder.fit(df['clothing_cat'].unique())\n",
        "  cond_encoder.fit(df['product_condition'].unique())\n",
        "  gen_encoder.fit(df['product_gender_target'].unique())\n",
        "  brand_encoder.fit(df['brand_name'].unique())\n",
        "  material_encoder.fit(df['product_material'].unique())\n",
        "\n",
        "  # encode variables\n",
        "  df['clothing_cat_encoded'] = cat_encoder.transform(df['clothing_cat'])\n",
        "  df['condition_encoded'] = cond_encoder.transform(df['product_condition'])\n",
        "  df['gender_encoded'] = gen_encoder.transform(df['product_gender_target'])\n",
        "  df['brand_encoded'] = brand_encoder.transform(df['brand_name'])\n",
        "  df['material_encoded'] = material_encoder.transform(df['product_material'])\n",
        "\n",
        "  return df, cat_encoder, cond_encoder, gen_encoder, brand_encoder, material_encoder\n",
        "\n",
        "standardized_df, cat_encoder, cond_encoder, gen_encoder, brand_encoder, material_encoder = standardize_data(target_df)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X = standardized_df[['clothing_cat_encoded', 'brand_encoded', 'condition_encoded', 'gender_encoded', 'material_encoded']]\n",
        "y = standardized_df['price_usd']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEHfwQEWOy79",
        "outputId": "84d6af61-9869-4ec8-d58a-e0943d8ae7d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "36021/36021 [==============================] - 129s 4ms/step - loss: 3472842.2500 - mean_absolute_error: 365.3486 - val_loss: 2242827.2500 - val_mean_absolute_error: 330.0871 - lr: 0.0010\n",
            "Epoch 2/25\n",
            "36021/36021 [==============================] - 132s 4ms/step - loss: 3438751.2500 - mean_absolute_error: 360.6555 - val_loss: 2254102.7500 - val_mean_absolute_error: 300.1667 - lr: 0.0010\n",
            "Epoch 3/25\n",
            "36021/36021 [==============================] - 145s 4ms/step - loss: 3417353.0000 - mean_absolute_error: 357.4487 - val_loss: 2215467.0000 - val_mean_absolute_error: 304.7891 - lr: 0.0010\n",
            "Epoch 4/25\n",
            "36021/36021 [==============================] - 137s 4ms/step - loss: 3395005.2500 - mean_absolute_error: 360.0306 - val_loss: 2196049.0000 - val_mean_absolute_error: 310.6590 - lr: 0.0010\n",
            "Epoch 5/25\n",
            "36021/36021 [==============================] - 134s 4ms/step - loss: 3396006.2500 - mean_absolute_error: 357.6193 - val_loss: 2146728.0000 - val_mean_absolute_error: 304.2143 - lr: 0.0010\n",
            "Epoch 6/25\n",
            "36021/36021 [==============================] - 136s 4ms/step - loss: 3360922.7500 - mean_absolute_error: 350.8620 - val_loss: 2181620.5000 - val_mean_absolute_error: 303.3205 - lr: 0.0010\n",
            "Epoch 7/25\n",
            "36021/36021 [==============================] - 135s 4ms/step - loss: 3342452.5000 - mean_absolute_error: 346.9853 - val_loss: 2236817.7500 - val_mean_absolute_error: 307.4799 - lr: 0.0010\n",
            "Epoch 8/25\n",
            "36021/36021 [==============================] - 143s 4ms/step - loss: 3333380.7500 - mean_absolute_error: 344.3887 - val_loss: 2123319.7500 - val_mean_absolute_error: 293.1033 - lr: 0.0010\n",
            "Epoch 9/25\n",
            "36021/36021 [==============================] - 146s 4ms/step - loss: 3343284.7500 - mean_absolute_error: 345.1961 - val_loss: 2206279.5000 - val_mean_absolute_error: 303.1225 - lr: 0.0010\n",
            "Epoch 10/25\n",
            "36021/36021 [==============================] - 140s 4ms/step - loss: 3326285.5000 - mean_absolute_error: 342.2006 - val_loss: 2098737.5000 - val_mean_absolute_error: 309.0026 - lr: 0.0010\n",
            "Epoch 11/25\n",
            "36021/36021 [==============================] - 146s 4ms/step - loss: 3307373.2500 - mean_absolute_error: 344.0951 - val_loss: 2056452.3750 - val_mean_absolute_error: 307.3669 - lr: 0.0010\n",
            "Epoch 12/25\n",
            "36021/36021 [==============================] - 155s 4ms/step - loss: 3320918.7500 - mean_absolute_error: 341.4308 - val_loss: 2078523.2500 - val_mean_absolute_error: 307.0872 - lr: 0.0010\n",
            "Epoch 13/25\n",
            "36021/36021 [==============================] - 145s 4ms/step - loss: 3294871.2500 - mean_absolute_error: 340.6042 - val_loss: 2114813.2500 - val_mean_absolute_error: 307.0173 - lr: 0.0010\n",
            "Epoch 14/25\n",
            "36021/36021 [==============================] - 145s 4ms/step - loss: 3301357.7500 - mean_absolute_error: 341.5352 - val_loss: 2106779.2500 - val_mean_absolute_error: 303.1073 - lr: 0.0010\n",
            "Epoch 15/25\n",
            "36021/36021 [==============================] - 151s 4ms/step - loss: 3301940.2500 - mean_absolute_error: 343.3422 - val_loss: 2108582.2500 - val_mean_absolute_error: 304.6715 - lr: 0.0010\n",
            "Epoch 16/25\n",
            "36021/36021 [==============================] - 140s 4ms/step - loss: 3299750.2500 - mean_absolute_error: 342.0084 - val_loss: 2093426.3750 - val_mean_absolute_error: 302.7986 - lr: 0.0010\n",
            "Epoch 17/25\n",
            "36021/36021 [==============================] - 153s 4ms/step - loss: 3203617.2500 - mean_absolute_error: 336.2402 - val_loss: 2011035.2500 - val_mean_absolute_error: 302.2298 - lr: 2.0000e-04\n",
            "Epoch 18/25\n",
            "36021/36021 [==============================] - 140s 4ms/step - loss: 3204244.2500 - mean_absolute_error: 334.2245 - val_loss: 1993528.7500 - val_mean_absolute_error: 304.0501 - lr: 2.0000e-04\n",
            "Epoch 19/25\n",
            "36021/36021 [==============================] - 144s 4ms/step - loss: 3196677.5000 - mean_absolute_error: 333.8035 - val_loss: 1998674.0000 - val_mean_absolute_error: 302.2460 - lr: 2.0000e-04\n",
            "Epoch 20/25\n",
            "36021/36021 [==============================] - 151s 4ms/step - loss: 3187200.2500 - mean_absolute_error: 332.4452 - val_loss: 2011053.0000 - val_mean_absolute_error: 298.4625 - lr: 2.0000e-04\n",
            "Epoch 21/25\n",
            "36021/36021 [==============================] - 144s 4ms/step - loss: 3188527.2500 - mean_absolute_error: 332.0132 - val_loss: 1988845.3750 - val_mean_absolute_error: 301.9662 - lr: 2.0000e-04\n",
            "Epoch 22/25\n",
            "36021/36021 [==============================] - 149s 4ms/step - loss: 3179464.5000 - mean_absolute_error: 331.2663 - val_loss: 1956509.2500 - val_mean_absolute_error: 301.0948 - lr: 2.0000e-04\n",
            "Epoch 23/25\n",
            "36021/36021 [==============================] - 140s 4ms/step - loss: 3159797.2500 - mean_absolute_error: 332.3340 - val_loss: 1956214.7500 - val_mean_absolute_error: 299.3467 - lr: 2.0000e-04\n",
            "Epoch 24/25\n",
            "36021/36021 [==============================] - 144s 4ms/step - loss: 3168078.7500 - mean_absolute_error: 331.5662 - val_loss: 1956814.5000 - val_mean_absolute_error: 304.0871 - lr: 2.0000e-04\n",
            "Epoch 25/25\n",
            "36021/36021 [==============================] - 155s 4ms/step - loss: 3172723.7500 - mean_absolute_error: 331.8287 - val_loss: 2001523.3750 - val_mean_absolute_error: 300.8713 - lr: 2.0000e-04\n",
            "5629/5629 [==============================] - 10s 2ms/step - loss: 11500358656.0000 - mean_absolute_error: 90690.3438\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[11500358656.0, 90690.34375]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# making the model\n",
        "num_features = X_train.shape[1]\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(num_features, activation='relu'),\n",
        "    tf.keras.layers.Dense(264, activation='relu'),\n",
        "    tf.keras.layers.Dense(132, activation='relu'),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5), # add dropout layers to avoid overfitting\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1, activation='linear')\n",
        "])\n",
        "\n",
        "# scale features to help with learning\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# learning rate scheduler to improve performance\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
        "\n",
        "model.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics=['mean_absolute_error']) # use absolute error to reduce sensititivy to outliers\n",
        "\n",
        "# train the model\n",
        "model.fit(X_train_scaled, y_train, epochs=25, batch_size=16, verbose=1, validation_split=0.2, callbacks=[reduce_lr])\n",
        "\n",
        "# test the model\n",
        "model.evaluate(X_test, y_test, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbU2VF8qWdjE"
      },
      "outputs": [],
      "source": [
        "# and save it!\n",
        "model.save('/content/drive/MyDrive/Vestiaire Model/reluModel')\n",
        "\n",
        "# and load it back in again if needed\n",
        "#price_predictor = load_model('/content/drive/MyDrive/Vestiaire Model/reluModel')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEWxu0VeWnDB",
        "outputId": "2a62a6de-6185-411a-d6bd-813166516fe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 215ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[74066.32]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# use the model! still working on it\n",
        "\n",
        "# 'clothing_cat_encoded', 'brand_encoded', 'condition_encoded', 'gender_encoded', 'material_encoded'\n",
        "def predict_price(category: str, brand: str, condition: str, gender: str, material: str, model):\n",
        "  ''' condition: 'Never worn', 'Very good condition', 'Never worn, with tag',\n",
        "       'Good condition', 'Fair condition'\n",
        "      gender: 'Men', 'Women', 'Unisex'\n",
        "      brand : .........\n",
        "      category: 'skirt', 'jacket', 'pants', 'dress', 'shirt', 'swimwear', 'shorts',\n",
        "       'undergarment', 'sweater', 'jumpsuit', 'accessories', 'suit',\n",
        "       'shoes'\n",
        "    '''\n",
        "\n",
        "  #  1) encode all inputs for model\n",
        "  label_encoder = LabelEncoder()\n",
        "  cond_encoded, gen_encoded, brand_encoded, cat_encoded, mat_encoded = cond_encoder.transform([condition.lower()])[0], gen_encoder.transform([gender.lower()])[0], brand_encoder.transform([brand.lower()])[0], cat_encoder.transform([category.lower()])[0], material_encoder.transform([material.lower()])[0]\n",
        "\n",
        "  # 2) package encoded inputs together to feed into model\n",
        "  input_array = np.array([cat_encoded, brand_encoded, cond_encoded, gen_encoded, mat_encoded])\n",
        "\n",
        "  # 3) predict the price!\n",
        "  price = model.predict(input_array.reshape(1, -1))\n",
        "\n",
        "  return price\n",
        "\n",
        "predict_price('shoes', 'gucci', 'good condition', 'women', 'leather', model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}